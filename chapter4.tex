\chapter{Inference-Time Scaling Strategies}

Building upon the customized AIDE agent scaffold and the vLLM-powered local inference environment detailed in Chapter 3, this chapter focuses on the specific Inference-Time Scaling (ITS) strategies implemented and evaluated in this thesis. The core objective of integrating these strategies is to enhance the problem-solving capabilities, robustness, and overall performance of open-source Large Language Models when applied to the complex machine learning engineering tasks presented by the MLE-Bench.

Each of the following subsections will describe an ITS technique, briefly revisiting its theoretical underpinnings, and then detailing its concrete implementation within our modified AIDE framework. We will outline the specific algorithmic modifications, data flow, and key parameters associated with each strategy, providing the necessary context for understanding how these techniques augment the agent's reasoning and generation processes. The quantitative impact of these strategies on performance metrics will be presented and analyzed in Chapter 5.

The ITS strategies explored in this work include Self-Reflection (SR), Self-Consistency (SC), Iterative Self-Debugging (ISDb), Prompt/Code Chaining (PC), and Tree of Thoughts (ToT).

\section{Self-Reflection (SR)}

\section{Self-Consistency (SC)}

\section{Prompt/Code chaining (PC)}

\section{Iterative self debugging (ISDB)}

\section{Tree of Thoughts (ToT)}
