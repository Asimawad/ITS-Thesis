\chapter{Inference-Time Scaling Strategies}

Building upon the customized AIDE agent scaffold and the vLLM-powered local inference environment detailed in Chapter 3, this chapter focuses on the specific Inference-Time Scaling (ITS) strategies implemented and evaluated in this thesis. The core objective of integrating these strategies is to enhance the problem-solving capabilities, robustness, and overall performance of open-source Large Language Models when applied to the complex machine learning engineering tasks presented by the MLE-Bench.

Each of the following subsections will describe an ITS technique, briefly revisiting its theoretical underpinnings, and then detailing its concrete implementation within our modified AIDE framework. We will outline the specific algorithmic modifications, data flow, and key parameters associated with each strategy, providing the necessary context for understanding how these techniques augment the agent's reasoning and generation processes. The quantitative impact of these strategies on performance metrics will be presented and analyzed in Chapter 5.

The ITS strategies explored in this work include Self-Reflection (SR), Self-Consistency (SC), Iterative Self-Debugging (ISDb), Prompt/Code Chaining (PC), and Tree of Thoughts (ToT).

\section{Self-Reflection (SR)}

\subsection{Motivation and Conceptual Overview}

Initial baseline experiments utilizing the AIDE agent scaffold with various open-source DeepSeek language models (7B, 14B, and 32B parameters) revealed a significant performance differential when compared to larger, proprietary counterparts. A primary observation from the analysis of execution logs (see Appendix \ref{appendix:baseline_analysis}) was the frequent generation of buggy initial code drafts, particularly by the smaller 7B and 14B models. These errors often stemmed from fundamental issues such as incorrect file path specifications (e.g., for accessing input data or saving submission files), missing import statements for necessary libraries, misuse of common machine learning library APIs, or basic logical errors in the Python code.

Furthermore, a critical bottleneck appeared in the models' ability to effectively learn from the feedback provided by AIDE's designated feedback-generating LLM (o4-mini) during the standard debugging cycles. Attempts by the agent to correct its code based on this feedback often failed to address the root cause of the error, or, in some cases, introduced new, unrelated bugs. This indicated a challenge not only in the initial generation of functionally correct code but also in the subsequent iterative refinement process.

These observations—specifically the prevalence of rectifiable errors in initial drafts and the difficulties in standard feedback-driven debugging—motivated the implementation of Self-Reflection (SR) as an Inference-Time Scaling (ITS) strategy. Conceptually, Self-Reflection allows an LLM-based agent to critically evaluate its own generated output and then use this internal critique to guide a subsequent refinement attempt. This approach draws inspiration from frameworks such as Reflexion (Shinn et al., 2023), which leverages "verbal reinforcement" – linguistic feedback generated by an LLM about its own past actions – to enhance learning and improve performance on complex tasks. Similarly, research by Renze \& Guven (2024) has demonstrated that various forms of structured self-reflection can significantly improve LLM problem-solving. The central hypothesis for our work was that by introducing an explicit SR step, particularly after a failed code execution attempt, the DeepSeek models could be prompted to identify and rectify their own errors more effectively than through AIDE's standard feedback loop alone.

\subsection{Implementation of Two-Step Post-Execution Self-Reflection}

After evaluating several potential SR pipeline configurations, a two-step self-reflection process was selected for implementation. This process is triggered if, and only if, an initial code execution attempt by the AIDE agent is deemed "buggy" by the feedback-generating LLM. The SR mechanism is integrated into the main operational loop of the AIDE agent and is activated via a specific configuration parameter (ITS-Strategy = "self-reflection"). The same base coding LLM that generated the initial (buggy) code is utilized for both stages of the reflection process. The overall flow is depicted in Algorithm \ref{alg:self-reflection} and can be summarized as follows:

SR Trigger: Following a standard AIDE code generation action (whether an initial draft, a debug attempt, or an improvement attempt), the generated script is executed. The output of this execution, along with the code itself, is then analyzed by AIDE's feedback-generating LLM (o4-mini). If this feedback LLM flags the executed script as "buggy" (due to runtime errors, failure to produce a valid submission file, or other issues), the self-reflection sequence is initiated for the current AIDE step.

\subsection{Self-Reflection Mechanism}

This involves two sequential calls to the primary coding LLM:

\textbf{Step 1: Critique Generation (The "Reviewer" Persona)}

Objective: The LLM is prompted to critically review the previously generated buggy code.

Inputs: The prompt to the LLM includes the original buggy script, the terminal output from its failed execution, the textual analysis provided by the feedback-generating LLM (o4-mini), and the overall task description for context.

Instructions \& Output Format: The LLM is instructed to act as a senior peer reviewing the code. It must identify the mistakes and provide a textual response comprising two parts: a "Review" section that explains the identified errors, and a numbered "Instructions" section that lists concise, text-based steps for rectifying these errors. Crucially, the LLM is explicitly forbidden from generating any Python code during this critique phase. If, after its review, it believes no changes are necessary (despite the buggy flag from the feedback LLM), it is instructed to output a specific phrase: "No specific errors found requiring changes."

LLM Parameters: A higher-temperature sampling (e.g., 0.7, as configured for general coding tasks) was employed for this critique generation to encourage a more comprehensive and potentially diverse analysis of the errors.

\textbf{Step 2: Focused Code Revision (The "Coder" Persona)}

Objective: If the preceding critique stage produced actionable fix instructions (i.e., did not state "No specific errors found..."), the LLM is then tasked with applying only these textual instructions to revise the original buggy code.

Inputs: The prompt for this stage includes the original buggy script, the execution output and feedback analysis (for context), and, most importantly, the textual "Edit Instructions" generated in the critique (Step 1).

Instructions \& Output Format: The LLM is instructed to act as a precise coder. It must strictly adhere to the provided textual edit instructions, making only the specified minimal changes to the original code. It is directed not to alter other parts of the code, nor to reformat it, and to ignore any accidental code snippets that might have appeared in the edit instructions. The expected output is a single, complete Python script, prefixed with a comment indicating it's a revised version (e.g., \# Applying edits based on review.).

LLM Parameters: For this code revision stage, the agent's general coding temperature (e.g., 0.7) was utilized. While a lower temperature (e.g., 0.0) is often advocated for precise editing to ensure deterministic application of fixes, the current implementation maintained consistency with the agent's standard coding temperature. This parameter remains an avenue for potential future refinement.

\subsection{Integration and Re-Evaluation}

The output of the critique generation stage (the "reflection plan") and the (potentially) revised code from the revision stage are collected.

If the revision stage produces a valid script that is different from the original buggy code, this revised\_code replaces the buggy code for the current AIDE agent step.

This revised\_code is then immediately re-executed within the AIDE framework.

The outcome of this re-execution (including any new terminal output) is re-parsed by the feedback-generating LLM (o4-mini) to determine if the self-reflection process successfully resolved the bug(s) and to obtain a new performance metric if applicable.

The final status (buggy or not, metric value) of the code after this entire SR and re-evaluation sequence is what is recorded in the AIDE journal for that step and used for subsequent decision-making by the agent's search policy.

This "post-mortem" approach to self-reflection—triggering SR after a confirmed failure—was adopted because it allows the reflection process to be directly informed by concrete execution errors and an initial diagnosis from the feedback LLM. This targeted approach was hypothesized to be more effective than a pre-emptive reflection on all generated drafts, which might lead the LLM to "fix" non-existent issues or introduce new errors without the grounding of actual execution feedback. The qualitative impact of this SR strategy is discussed in Chapter 5.

Algorithm \ref{alg:self-reflection} outlines the two-step post-execution self-reflection process within the AIDE framework.

\begin{algorithm}
\caption{Two-Step Post-Execution Self-Reflection within AIDE}
\label{alg:self-reflection}
\end{algorithm}

\section{Self-Consistency (SC)}

\section{Prompt/Code chaining (PC)}

\section{Iterative self debugging (ISDB)}

\section{Tree of Thoughts (ToT)}
