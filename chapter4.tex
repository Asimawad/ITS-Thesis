\chapter{Inference-Time Scaling Strategies}

Building upon the customized AIDE agent scaffold and the vLLM-powered local inference environment detailed in Chapter 3, this chapter focuses on the specific Inference-Time Scaling (ITS) strategies implemented and evaluated in this thesis. The core objective of integrating these strategies is to enhance the problem-solving capabilities, robustness, and overall performance of open-source Large Language Models when applied to the complex machine learning engineering tasks presented by the MLE-Bench.

Each of the following subsections will describe an ITS technique, briefly revisiting its theoretical underpinnings, and then detailing its concrete implementation within our modified AIDE framework. We will outline the specific algorithmic modifications, data flow, and key parameters associated with each strategy, providing the necessary context for understanding how these techniques augment the agent's reasoning and generation processes. The quantitative impact of these strategies on performance metrics will be presented and analyzed in Chapter 5.

The ITS strategies explored in this work include Self-Reflection (SR), Self-Consistency (SC), Iterative Self-Debugging (ISDb), Prompt/Code Chaining (PC), and Tree of Thoughts (ToT).

\section{Self-Reflection (SR)}

\subsection{Motivation and Conceptual Overview}

Initial baseline experiments utilizing the AIDE agent scaffold with various open-source DeepSeek language models (7B, 14B, and 32B parameters) revealed a significant performance differential when compared to larger, proprietary counterparts. A primary observation from the analysis of execution logs (see Appendix \ref{appendix:baseline_analysis}) was the frequent generation of buggy initial code drafts, particularly by the smaller 7B and 14B models. These errors often stemmed from fundamental issues such as incorrect file path specifications (e.g., for accessing input data or saving submission files), missing import statements for necessary libraries, misuse of common machine learning library APIs, or basic logical errors in the Python code.

Furthermore, a critical bottleneck appeared in the models' ability to effectively learn from the feedback provided by AIDE's designated feedback-generating LLM (o4-mini) during the standard debugging cycles. Attempts by the agent to correct its code based on this feedback often failed to address the root cause of the error, or, in some cases, introduced new, unrelated bugs. This indicated a challenge not only in the initial generation of functionally correct code but also in the subsequent iterative refinement process.

These observations—specifically the prevalence of rectifiable errors in initial drafts and the difficulties in standard feedback-driven debugging—motivated the implementation of Self-Reflection (SR) as an Inference-Time Scaling (ITS) strategy. Conceptually, Self-Reflection allows an LLM-based agent to critically evaluate its own generated output and then use this internal critique to guide a subsequent refinement attempt. This approach draws inspiration from frameworks such as Reflexion (Shinn et al., 2023), which leverages "verbal reinforcement" – linguistic feedback generated by an LLM about its own past actions – to enhance learning and improve performance on complex tasks. Similarly, research by Renze \& Guven (2024) has demonstrated that various forms of structured self-reflection can significantly improve LLM problem-solving. The central hypothesis for our work was that by introducing an explicit SR step, particularly after a failed code execution attempt, the DeepSeek models could be prompted to identify and rectify their own errors more effectively than through AIDE's standard feedback loop alone.

\subsection{Implementation of Two-Step Post-Execution Self-Reflection}

After evaluating several potential SR pipeline configurations, a two-step self-reflection process was selected for implementation. This process is triggered if, and only if, an initial code execution attempt by the AIDE agent is deemed "buggy" by the feedback-generating LLM. The SR mechanism is integrated into the main operational loop of the AIDE agent and is activated via a specific configuration parameter (ITS-Strategy = "self-reflection"). The same base coding LLM that generated the initial (buggy) code is utilized for both stages of the reflection process. The overall flow is depicted in Algorithm \ref{alg:self-reflection} and can be summarized as follows:

SR Trigger: Following a standard AIDE code generation action (whether an initial draft, a debug attempt, or an improvement attempt), the generated script is executed. The output of this execution, along with the code itself, is then analyzed by AIDE's feedback-generating LLM (o4-mini). If this feedback LLM flags the executed script as "buggy" (due to runtime errors, failure to produce a valid submission file, or other issues), the self-reflection sequence is initiated for the current AIDE step.

\subsection{Self-Reflection Mechanism}

This involves two sequential calls to the primary coding LLM:

\textbf{Step 1: Critique Generation (The "Reviewer" Persona)}

Objective: The LLM is prompted to critically review the previously generated buggy code.

Inputs: The prompt to the LLM includes the original buggy script, the terminal output from its failed execution, the textual analysis provided by the feedback-generating LLM (o4-mini), and the overall task description for context.

Instructions \& Output Format: The LLM is instructed to act as a senior peer reviewing the code. It must identify the mistakes and provide a textual response comprising two parts: a "Review" section that explains the identified errors, and a numbered "Instructions" section that lists concise, text-based steps for rectifying these errors. Crucially, the LLM is explicitly forbidden from generating any Python code during this critique phase. If, after its review, it believes no changes are necessary (despite the buggy flag from the feedback LLM), it is instructed to output a specific phrase: "No specific errors found requiring changes."

LLM Parameters: A higher-temperature sampling (e.g., 0.7, as configured for general coding tasks) was employed for this critique generation to encourage a more comprehensive and potentially diverse analysis of the errors.

\textbf{Step 2: Focused Code Revision (The "Coder" Persona)}

Objective: If the preceding critique stage produced actionable fix instructions (i.e., did not state "No specific errors found..."), the LLM is then tasked with applying only these textual instructions to revise the original buggy code.

Inputs: The prompt for this stage includes the original buggy script, the execution output and feedback analysis (for context), and, most importantly, the textual "Edit Instructions" generated in the critique (Step 1).

Instructions \& Output Format: The LLM is instructed to act as a precise coder. It must strictly adhere to the provided textual edit instructions, making only the specified minimal changes to the original code. It is directed not to alter other parts of the code, nor to reformat it, and to ignore any accidental code snippets that might have appeared in the edit instructions. The expected output is a single, complete Python script, prefixed with a comment indicating it's a revised version (e.g., \# Applying edits based on review.).

% LLM Parameters: For this code revision stage, the agent's general coding temperature (e.g., 0.7) was utilized. While a lower temperature (e.g., 0.0) is often advocated for precise editing to ensure deterministic application of fixes, the current implementation maintained consistency with the agent's standard coding temperature. This parameter remains an avenue for potential future refinement.

\subsection{Integration and Re-Evaluation}

The output of the critique generation stage (the "reflection plan") and the (potentially) revised code from the revision stage are collected.

If the revision stage produces a valid script that is different from the original buggy code, this revised\_code replaces the buggy code for the current AIDE agent step.

This revised\_code is then immediately re-executed within the AIDE framework.

The outcome of this re-execution (including any new terminal output) is re-parsed by the feedback-generating LLM (o4-mini) to determine if the self-reflection process successfully resolved the bug(s) and to obtain a new performance metric if applicable.

The final status (buggy or not, metric value) of the code after this entire SR and re-evaluation sequence is what is recorded in the AIDE journal for that step and used for subsequent decision-making by the agent's search policy.

This "post-mortem" approach to self-reflection—triggering SR after a confirmed failure—was adopted because it allows the reflection process to be directly informed by concrete execution errors and an initial diagnosis from the feedback LLM. This targeted approach was hypothesized to be more effective than a pre-emptive reflection on all generated drafts, which might lead the LLM to "fix" non-existent issues or introduce new errors without the grounding of actual execution feedback. The qualitative impact of this SR strategy is discussed in Chapter 5.

Algorithm \ref{alg:self-reflection} outlines the two-step post-execution self-reflection process within the AIDE framework.

\begin{algorithm}
\caption{Two-Step Post-Execution Self-Reflection within AIDE}
\label{alg:self-reflection}
\end{algorithm}

\section{Task Decomposition via Planner-Coder and Chained Modular Generation}

To address the inherent difficulty of generating complex, end-to-end machine learning solutions in a single LLM pass, a significant ITS strategy explored in this research involves task decomposition. This approach breaks down the monolithic code generation problem into more manageable sub-problems, inspired by established reasoning frameworks like ReAct (Yao et al., 2023) and plan-then-act pipelines in code generation (Zhang et al., 2023; Wang et al., 2023). The core idea is to separate strategic planning from detailed code implementation, thereby improving focus, reducing error propagation, and allowing for more targeted interventions.

This strategy was implemented in two main variants within the AIDE scaffold: a PlannerAgent and a more sophisticated CodeChainAgent. Both leverage a conceptual split between a "Planner" role, responsible for high-level strategy, and a "Coder" role, responsible for implementation.

\subsection{The Planner-Coder Paradigm: Separating Thought from Action}
Analogous to the ReAct framework where an agent alternates between "Thought" (reasoning and planning) and "Action" (interacting with an environment or tools), our Planner-Coder paradigm introduces a two-stage process for each AIDE operation (draft, improve, debug):

Planning Stage: An LLM designated as the "Planner" first receives the task context (task description, data preview, memory of previous attempts, or existing code to be improved/debugged). Its objective is to produce a structured high-level plan. For drafting new solutions, this involves generating a "Task Summary" and a detailed, step-by-step Master Plan. For improving or debugging, it involves analyzing the existing code/error and formulating a specific "Improvement Plan" or "Fix Plan." This stage is facilitated by dedicated system and user prompts (e.g., get_planner_agent_plan_system_prompt and prompts like get_planner_agent_draft_plan_user_prompt) designed to elicit strategic, declarative instructions rather than executable code. (The full prompt structures are detailed in Appendix [Your Appendix Letter/Number for Prompts]).

Coding Stage: The plan generated by the Planner is then passed to a separate LLM call, designated as the "Coder." The Coder's sole responsibility is to translate the provided plan into executable Python code. It receives the plan, relevant context (like the Task Summary or previous code), and is prompted (e.g., using get_planner_agent_code_system_prompt and prompts like get_planner_agent_draft_code_user_prompt) to strictly adhere to the plan and generate the complete Python script.

This separation allows for potentially different models or configurations to be used for planning versus coding. Experiments were conducted using both the same model for both roles and configurations where a model strong in reasoning (e.g., a general DeepSeek R1 variant) handled planning, while a model specialized in code generation (e.g., a DeepSeek Coder variant) handled the implementation. A planner_flag parameter within the LLM query mechanism internally managed the routing or role-specific prompting when distinct models or specialized system prompts were employed. The PlannerAgent class implemented this two-stage Planner-then-Coder approach for all its _draft, _improve, and _debug methods, with the Coder generating the full script in one go after receiving the plan.

\subsection{CodeChainAgent: Modular Code Generation with Optional Reflection}
The CodeChainAgent, which inherits directly from the base Agent class and overrides its core methods, takes the task decomposition further, particularly for the _draft operation. Instead of the Coder generating the entire script based on the Master Plan, the CodeChainAgent implements a chained, modular generation process:

Master Plan Generation: Similar to PlannerAgent, an initial call to the Planner LLM generates a "Task Summary" and a "Master Plan."

Segmented Code Generation (_draft\_generate\_code\_chained): The Master Plan is then conceptually divided into a sequence of predefined, logical code segments (e.g., "Setup \& Imports," "Data Loading," "Data Preprocessing," "Modeling," "Training \& Validation," "Prediction \& Submission"). The Coder LLM is invoked iteratively for each segment.

For each segment, the Coder receives the task_summary, the full master_plan_text, and all code generated in previous segments (code_accumulator). It is then prompted (using segment-specific system and user prompts, e.g., CHAINED\_CODER\_SYSTEM\_PROMPT\_GETTERS and CHAINED\_CODER\_USER\_PROMPT\_CONSTRUCTORS) to generate only the code snippet pertaining to the current segment.

This newly generated snippet is appended to the code_accumulator, which is then passed as context for generating the next segment. This iterative process aims to maintain context and ensure continuity across the generated script.

\subsection{Integrated Reflection Mechanisms (Configurable via agent.ITS\_Strategy)}
To enhance the quality of the incrementally built code, the CodeChainAgent can incorporate reflection steps:

Segment-Level Reflection (codechain\_v2 strategy): After each segment's code snippet is initially generated, an additional LLM call (_reflect\_on\_segment) is made. This "Reflector" LLM (which could be the same model as the Coder or Planner, guided by specific reflection prompts like get\_segment\_reflection\_system\_prompt) reviews the initial\_code\_snippet\_for\_this\_segment in the context of the master\_plan\_text and code\_generated\_before\_this\_segment. It then produces a "Reflection Summary" and potentially a "Revised Code Snippet" for that segment, which replaces the original if changes are made.

Chunk-Level Reflection (codechain\_v3 strategy): To allow for reflection over a broader context, this strategy groups segments into "chunks" (e.g., 2 segments per chunk). Code for all segments in a chunk is generated sequentially first. Then, a single reflection call (_reflect\_on\_chunk) is made for the entire chunk's combined code. The Reflector LLM receives context including the code generated before the current chunk and the concatenated initial\_chunk\_code. It produces a summary and a revised code block for the entire chunk, which then updates the code\_accumulator.

The _improve and _debug methods within CodeChainAgent revert to the simpler Planner-then-full-Coder approach of PlannerAgent, as the chained generation with reflection is primarily targeted at the more open-ended _draft phase. The final Node produced by any of these operations (draft, improve, or debug) is then processed by the standard AIDE execution and feedback loop.

This decomposed, multi-stage generation, especially when combined with reflection, aims to improve the LLM's ability to manage complexity, maintain long-range dependencies, and correct errors early, thereby enhancing the overall quality and success rate of the generated machine learning solutions. The explicit separation of planning and coding, and the further modularization in CodeChainAgent, provides distinct intervention points for applying various reasoning and refinement techniques.


\section{Self-Consistency for Robust Code Generation}

To enhance the robustness and quality of solutions generated by AIDE, particularly in the face of the inherent stochasticity of Large Language Models (LLMs), the principle of Self-Consistency (SC) was integrated into the agent's decision-making process. This approach draws directly on recent research demonstrating significant performance gains in LLM reasoning by sampling multiple independent reasoning traces and selecting the most plausible or highest-quality output (Wang et al., 2023b). While initial work focused on chain-of-thought reasoning for textual answers, subsequent studies like STaR (Self-Taught Reasoner, Zelikman et al., 2022), Vote-N-Verify (Deng et al., 2023), and Reflexion (Shinn et al., 2023) further refined this paradigm with verifier-based scoring or iterative reflection.

Our SelfConsistencyAgent generalizes this concept from textual reasoning to the generation of executable machine learning pipelines. The core idea remains: sample multiple candidate solutions from the LLM's output distribution for a given prompt and then employ a selection mechanism, informed by execution feedback, to choose the most promising one. This aims to mitigate the impact of occasional suboptimal or erroneous generations by exploring a broader set of potential outputs.

\subsection{SC Integration Across AIDE Operations}
Self-Consistency was systematically applied to all three primary generative operations within AIDE: drafting initial solutions (_draft), improving existing non-buggy solutions (_improve), and debugging faulty solutions (_debug). This was achieved by creating a SelfConsistencyAgent that inherits from the base Agent class and overrides these core methods.

Configuration: The number of candidate solutions (N) to generate for SC is a configurable parameter (agent.selfConsistency.num\_responses, typically 3 or 5). Crucially, a non-zero temperature (e.g., 0.7 to 0.99, via agent.code.temp) was used during the LLM queries for these operations to ensure diversity among the N generated candidates, mirroring the stochastic decoding used in the original SC work (Wang et al., 2023b).

\subsection{SC Workflow and Task-Aware Selection Rules}
The SC workflow adapts slightly depending on the AIDE operation:

For Drafting New Solutions (SelfConsistencyAgent.\_draft):

A single "Task Summary" and "Master Plan" are first generated by a Planner LLM.

This common plan is then provided to the Coder LLM, which is prompted N times to produce N diverse Python code scripts, each attempting to implement the same Master Plan.

Each of these N code scripts is executed by AIDE's interpreter, and its output (including any errors and the validation metric if successful) is parsed by the feedback LLM. This execution and evaluation process acts as a much stricter "verifier" than the lightweight checkers or gold labels used in some earlier SC applications for text or math problems.

The final solution added to AIDE's journal is selected from these N evaluated candidates based on a chosen selection_strategy:

interpreter_first_success: This strategy emulates the spirit of majority voting for correctness found in Wang et al. (2023b) by selecting the first candidate script that executes without bugs and successfully produces a valid submission file.

interpreter_best_metric: This strategy aligns more closely with verifier-based scoring approaches like Vote-N-Verify (Deng et al., 2023), selecting the non-buggy candidate that achieves the optimal validation metric as judged by the feedback LLM and AIDE's metric comparison logic.

For Improving and Debugging Solutions (SelfConsistencyAgent._improve / _debug):
The SC mechanism is applied to the generation of N complete (plan, code) pairs.

The LLM (using the Planner-then-Coder pattern for each of the N samples, or a combined plan-and-code generation if configured) is prompted N times to produce N diverse suggestions, each comprising a specific plan for improvement/debugging and the corresponding full Python code implementing that plan.

Each of these N (plan, code) pairs is then executed and evaluated.

The selection of the final (plan, code) pair to become the child node in AIDE's journal follows the same selection_strategy ("interpreter\_first\_success" or "interpreter\_best\_metric") as in the drafting phase.

\subsection{Extending Prior Work}
This implementation of Self-Consistency within AIDE extends prior work in several key ways:

From Answers to Executable Programs: Unlike earlier SC applications that often judged samples against simple gold labels or regular expressions, our agent executes every candidate script in a sandboxed environment. This allows for the detection of runtime errors and, more importantly, the evaluation of genuine machine learning performance via metrics, providing a significantly more rigorous filtering mechanism.

Unified Application Across the Agent Lifecycle: While many SC studies focus on single-shot reasoning or question-answering, our approach integrates the N-way sampling and verifier loop as a "meta-policy" across all generative stages (draft, improve, debug) of AIDE’s iterative tree search. This makes SC a fundamental, reusable component of the agent's problem-solving strategy.

Contextual Link to Iterative Refinement: While Reflexion (Shinn et al., 2023) also involves execution and reflection on failures, our SC pipeline acts as a robust initial selection mechanism. If the chosen self-consistent candidate is still buggy (especially in the _draft stage), it can then become the input for subsequent ITS techniques like Self-Reflection, as implemented in other agent variants within this thesis.

By systematically generating and verifying multiple solution pathways, the SelfConsistencyAgent aims to significantly increase the likelihood of AIDE producing a high-quality, non-buggy solution at each step. This marries the robustness benefits of self-consistency with AIDE’s code-execution feedback loop, yielding a principled, task-agnostic inference-time scaling strategy, particularly beneficial for enhancing the reliability of smaller open-source models in complex ML engineering tasks.


\subsection{Iterative Self-Debugging (ISDB)}

While AIDE's default workflow includes a single "debug" operation for buggy nodes, complex errors or subtle bugs may not be resolved in one attempt. To address this, an Iterative Self-Debugging (ISDb) strategy was implemented within a SelfDebugAgent. This approach is directly inspired by the $S*$ paradigm (Jiang et al., 2023), which demonstrated that enabling an LLM to run code, inspect feedback (e.g., tracebacks), and repeatedly propose minimal fixes can significantly improve reliability and achieve "software-grade" code without requiring model parameter updates. Instead of relying on a single prompt to yield perfect code, $S*$ (and our ISDb implementation) wraps the model in a tight execute → analyze error → patch loop.

\subsection{ISDb Integration into the AIDE Step}
Our SelfDebugAgent embeds this S*-inspired iterative loop directly within a single AIDE agent step. After an initial candidate script is generated via AIDE's standard _draft, _improve, or _debug operations:

        Initial Execution and Judgment: The candidate script is executed, and its output (traceback, validation metric) is parsed by the feedback LLM to determine if it's bug-free and produces a valid submission.

        Success Condition: If the initial execution is successful, the node is committed to the AIDE journal as is, and the ISDb loop is bypassed for this step.

        Iterative Debugging Loop Activation: If the initial execution fails (i.e., the node is buggy), the ISDb loop is initiated for up to a pre-configured maximum number of rounds (K, typically set to 3 in our experiments).

            In each round r = 1 … K:

                AIDE's standard _debug() method is invoked. It receives the current state of the buggy node, including the most recent error log and the feedback LLM's analysis. The _debug() method prompts the LLM to analyze this specific error and propose a minimal patch to fix it, generating a new (patched) version of the script.

                This patched script is then immediately re-executed and judged.

                Each such debug attempt (the patched script and its execution outcome) is appended to the AIDE journal to maintain a transparent "treatment history" and full lineage.

                If this new run is successful (not buggy), the ISDb loop halts. The fixed node (with the successfully patched code and its new evaluation metrics) replaces the initially buggy candidate for the current AIDE step and is committed to the journal.

                If the patched script is still buggy, its new error trace and analysis update the state of the node, which then becomes the input for the next ISDb round (N ← N' in your Algorithm ★).

        Loop Termination: If, after K rounds, the node remains buggy, it is committed to the journal in its last buggy state.

    This process is summarized in Algorithm [Reference to Your Algorithm ★, e.g., Algorithm 2].

        
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{parent node P (may be None), max\_rounds K}
\KwOut{final node N for the AIDE step}
    N \leftarrow \text{initial\_candidate\_generation}(P) \tcp{draft/improve/debug}
    (N, \text{is\_ok}) \leftarrow \text{execute\_and\_judge}(N) \\
    \If{\text{is\_ok}}{
    \Return{N} \\
    }
    \For{r = 1 \ldots K}{
    N'_{\text{candidate}} \leftarrow \text{\_debug}(N) \tcp{LLM proposes patch based on N's error}
    (N'_{\text{candidate}}, \text{is\_ok}') \leftarrow \text{execute\_and\_judge}(N'_{\text{candidate}}) \\
    \text{journal.append}(N'_{\text{candidate}}) \tcp{Record attempt}
    \If{\text{is\_ok}'}{
    \Return N'_{\text{candidate}} \tcp{Success, commit fixed version}
    }
    N \leftarrow N'_{\text{candidate}} \tcp{Carry latest error forward for next attempt}
    }
    \Return N \tcp{Still buggy after K rounds}
\caption{S*-style Iterative Debugging inside an AIDE Step}
\label{alg:s_star_debug}
\end{algorithm}

        

\subsection{Design Choices and Rationale}
Several design choices were made to optimize the ISDb process:

    Minimal-Edit Bias: Consistent with S*'s emphasis on locality, each debug prompt within the loop instructs the LLM to focus on fixing only the lines of code directly implicated by the most recent traceback. This aims to mitigate the risk of catastrophic rewrites that might regress performance or introduce new, unrelated bugs.

    Early-Exit Efficiency: The ISDb loop terminates on the first successful execution of a patched script. This contrasts with strategies like Self-Consistency (Section Y.Y) where multiple candidates might be fully evaluated. Empirical profiling suggested this early-exit approach could significantly reduce average GPU time, especially with smaller models.

    Optional Interaction with Self-Reflection: For agent configurations that also include Self-Reflection (Section Y.W), a one-shot reflection step can optionally be performed before initiating the ISDb loop if the initial candidate is buggy. The rationale is that reflection might resolve simpler oversights or high-level plan errors more cheaply, reserving the more granular S*-style iterations for genuinely non-trivial execution bugs.

    Functionality-Focused Stopping (Metric-Awareness): While the primary goal of ISDb is to achieve a runnable, non-buggy script, the evaluation still occurs. If a patched candidate executes without error but yields a significantly worse validation metric than a non-buggy parent (in an _improve scenario, for instance), AIDE's standard tree search mechanisms might still discard this path in later steps. The immediate goal of ISDb itself, however, is functional correctness; performance optimization is largely deferred to subsequent AIDE _improve operations on now-working code.

\subsection{Expected Impact}
The ISDb strategy directly targets a principal failure mode observed with the 7B-14B parameter open-source models used in this research: the generation of syntactically plausible code that nonetheless fails during execution due to runtime errors (e.g., incorrect API usage, path errors, import mistakes). By transforming compiler and runtime feedback into an inner corrective loop within a single AIDE step, ISDb is hypothesized to significantly increase the Valid Submission Rate. This is anticipated to be particularly impactful for tasks or models where the first-try success rate of the baseline agent is low, effectively salvaging more attempts and allowing AIDE to explore a wider range of viable solution paths.

\section{Tree of Thoughts (ToT)}
