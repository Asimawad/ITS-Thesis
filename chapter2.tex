% % \documentclass[11pt]{article}
% % \usepackage{amsmath,amssymb,amsfonts,amsthm,latexsym} 
% % \usepackage[a4paper,text={6.5in,9in}]{geometry} 
% % \begin{document}

% \chapter{Background and Related Work}

% \section{Background}

% \subsection{Large Language Models (LLMs)}
% Large Language Models (LLMs) are neural networks trained on extensive text datasets, capable of generating human-like text. Based primarily on the transformer architecture, these models utilize a straightforward predictive objective, leveraging enormous volumes of data to tackle complex problems and diverse natural language tasks [CITATION], including translation [CITATION], semantic analysis [CITATION], and information retrieval.

% Recent advancements in LLMs, exemplified by models such as the GPT series [CITATION*3] and the Llama family [CITATION], have been driven by significant scale, reaching billions of parameters. This substantial scale has unlocked emergent abilities—advanced skills like reasoning and in-context learning—that smaller models typically do not exhibit. These emergent capabilities enable LLMs to solve arithmetic problems, answer intricate questions, and summarize complex passages, often without explicit task-specific training.

% \subsection{Scaling Laws and Train-Time Compute}
% Scaling lows shows how the performance of Large Language models improves in a predeictable way with the increase in the parameter size, training data and computationa; resources, openai ahs shown that LLMs performance as measured by metrics such as accuracy or perplexity follows a power low relationship with these scaling factors. Performance enhancements are steady yet exhibit diminishing returns as scale increases, motivating the development of increasingly larger models like PaLM and Chinchilla.

% While scaling laws provided performance guarantees and drove a competitive race for building larger models, leading to current models in the hundreds of billions of parameters, practical limitations have surfaced. High-quality data is becoming scarce relative to growing model demands, and the significant costs and resource consumption associated with these models pose substantial challenges. Moreover, LLMs auto-regressive and token level prediction style is itself a limitation when it comes to complex reasoning tasks such as advanced mathematics or complex question answering. While recent years witnessed substantial developments in the models abilities, especially using post training and fine tuning, this requires massive computational resources and large scale datasets, encouraging researchers to to find cheaper approaches.

% \subsection{Inference-Time Scaling (ITS)}
% To address limitations associated with traditional scaling strategies, a new paradigm has emerged: inference-time scaling, also known as test-time scaling. Instead of allocating more resources to pretraining, this approach dynamically adjusts computational resources during inference, enabling models to "think longer" and thus handle more complex problems. Inference-time scaling involves techniques such as generating longer reasoning chains, sampling multiple possible answers, and iteratively refining outputs. This strategy has shown significant promise, allowing smaller models to surpass larger counterparts.

% Techniques in inference-time scaling include:
% \begin{itemize}
% \item \textbf{Chain-of-Thought Prompting (CoT):} Encouraging models to explicitly generate intermediate reasoning steps.
% \item \textbf{Iterative Self-Refinement:} Models iteratively identify and correct errors in their outputs.
% \item \textbf{Self-Consistency:} Generating multiple independent outputs and selecting the most consistent result via voting.
% \item \textbf{Verifier-Guided Search Techniques:} Utilizing reward-based verifiers or structured search methods to identify optimal answers from multiple generated solutions.
% \end{itemize}

% \subsection{Emergence of Reasoning Models}
% Recent developments have highlighted the emergence of models focused on reasoninging specifically designed to enhanthe capabilities in multistep  logical and mathematical reasoning. These reasoning models utilize inference-time strategies like structured prompting, external tool usage, and systematic search without altering model weights. Techniques such as ReAct, PAL, Toolformer, and Tree-of-Thoughts have demonstrated significant improvements in reasoning abilities.

% DeepSeek-R1 serves as a notable landmark, explicitly trained via reinforcement learning on structured reasoning datasets to generate coherent reasoning chains, significantly improving performance on complex reasoning tasks. DeepSeek's release of smaller distilled models, trained on datasets generated by R1, demonstrates the potential of distillation combined with structured inference strategies, achieving performance comparable or superior to larger models.

% * the effect of rl training on producing long chains of thoughts
% * the distilled models that were trained on the reasoning datata from R1 model

% \subsection{Agentic Systems and Scaffolding}
% "Scaffolding" reffers to code built up around an LLM in order to augment its capabilities. This does not typically include code which alters the LLM's internals, such as fine-tuning or activation steering. People use scaffolding because it can allow LLMs to use tools, reduce error rates, search for info, all of which make LLMs more capable.
% Common types of scaffolds include:
% Prompt templates: Perhaps the simplest scaffolds, "prompt templates" are just prompts with some blank sections to be filled in at runtime using some local data. E.g., a template like "You are a good assistant. The date is {INSERT DATE HERE}. Tell me how many days are left in the month." can be filled in with the current date before being used to prompt an LLM.
% Retrieval Augmented Generation (RAG): For tasks which have some associated data, RAG is a way of automatically adding task-relevant info to prompts. RAG does this by looking for snippets of text/data in the database which are relevant to the user's prompt, and then adding those snippets to the prompt.
% Search engines: Similar to RAG, giving an LLM access to a search engine lets it find info relevant to its prompt. Unlike RAG, the LLM decides what to search for and when.
% Agent scaffolds: Scaffolds which try to make LLMs into goal-directed agents, i.e., giving an LLM some goal/task to perform, and ways to observe and act on the world. Usually, the LLM is put into a loop with a history of its observations and actions, until its task is done. Some agent frameworks give LLMs access to much more powerful high-level actions, which saves the AI from repeating many simple primitive actions and, therefore, speeds up planning.

% Agent scaffolds are characterized by their capacity for perception, planning, and action within an environment, often governed by an iterative process that refines, reflects, and improves the solution over multiple cycles.

% While agents scaffolds can be used in different domains and wide array of tasks, there has been increasing interest dedicated to developing agents capable of automating the machine learning research and engineering process. A notable example is the [AI scientist] system, which presents an end-to-end pipeline designed to automate the research workflow. Currently, data science, machine learning engineering, and more generally, coding agents and assistants have shown consistent improvements. as we now see agents like Gihub Capilot, Cognition | Introducing Devin, Cursor, or Claude code exhibit phenomenon coding abilities, but there is no significant progress in their open-source coding agents. 

% The nature of coding problems presents a unique challenge for LLMs compared to other reasoning tasks, such as mathematics. In mathematical reasoning, there is often a single, unambiguously verifiable answer that can be derived through logical deduction. Conversely, generating a coding solution involves not only producing syntactically correct and executable code, but also navigating a vast solution space, where multiple functionally equivalent but structurally diverse implementations can exist. Crucially, the correctness and optimality of a coding solution often necessitate empirical validation through actual code execution and looking at traceback, rather than purely symbolic or logical verification. This inherent complexity, which demands iterative refinement and empirical testing, underscores the critical need for sophisticated agentic systems that can autonomously generate, execute, and iteratively refine solutions, even for highly capable foundational models.

% \subsection{Relevance and Challenges in Coding and ML Engineering Tasks}
% Machine learning engineering and coding tasks represent particularly challenging yet highly relevant applications for inference-time scaling due to their inherent complexity and precision requirements. Benchmarks like HumanEval, MBPP, and MLE-Bench exemplify these tasks, providing challenging evaluation contexts.

% However, significant gaps persist in current research, such as limited open-source implementations and insufficient evaluation on realistic, complex coding tasks. This project explicitly addresses these gaps by systematically applying inference-time scaling techniques to small-scale distilled reasoning models, aiming to substantially enhance their performance and practical applicability in coding and ML engineering domains.

% CITATION NEEDED
% \section{Related Work}
% * systems such as aide, or any open source implemntaion
% * infercne time and how it improves small models
% *** I should find example for MATH of course ***
 
% % **Refine this thing**
% % \subsection{Inference-Time Scaling Techniques in the Coding Domain}

% The core idea in Inference-Time Scaling (ITS) for code generation is to allow a model to generate and evaluate multiple solutions or reasoning steps before finalizing an answer, much like a programmer iteratively writes, tests, and debugs code. This can enable smaller models to achieve results on par with much larger models by compensating with clever search and reasoning. Below is an overview of relevant ITS
% strategies in the context of code generation:

% \subsection{Multiple Sampling and Majority Voting}:
% The simplest form of test-time compute scaling is to sample many outputs and pick the best, much like an ensemble methods, this can be very effective as the LLMs output are not generally deterministic, and in theory. For coding, this might mean prompting a 7B model 10 times for a function implementation and then using a heuristic to choose among those attempts. Majority voting is one approach where if multiple samples converge on the same answer, that answer is taken as more likely correct. However, for complex coding problems (especially ones without a single "exact" answer), majority voting may not help much – if the model tends to make a consistent mistake, all samples will be wrong in the same way. Nonetheless, for simpler tasks (like leetcode-style problems with fixed outputs), self-consistency via voting can reduce random errors. 

% A more effective variant is "Best-of-N" selection, where instead of voting, the best candidate out of N is chosen based on an evaluation function. In coding, the evaluation function could be a set of unit tests or a code quality metric. For example, generate 5 different code solutions and then run all of them against some test cases, picking the one that passes the most tests. 

% This Best-of-N strategy was found to significantly improve math problem solving for small
% models when using an automated judge, and for code it can similarly boost correctness if a reliable evaluator is available. In practice, Best-of-N requires more inference runs (linear in N) and possibly running a reward model or tests to rank outputs, but it's a straightforward way to leverage extra compute. 

% Iterative Self-Improvement (Self-Reflection and Debugging): instead of generating
% many answers independently, another approach is to have the model improve
% a single solution by self-correcting mistakes in an iterative way. Techniques like self-
% reflection, as well as Self-Debugging and Reflexion feedback loops.

% The general pattern is: the model produces an initial solution, then the model (or another instance of it or another role controlled by the system prompt) is asked to inspect that solution for errors or flaws, and propose a fix, and this process repeats. In code generation, this mirrors the edit-compile-fix cycle programmers use. For example, Chen et al. (2023) introduced Self-Debugging, where an LLM is taught via examples to run the code (or at least simulate running it) and then asked to explain what the code is doing and identify any mistakes based on either runtime results or logical reasoning. Notably, their approach has the model look at execution results (e.g. error messages or failed tests) without human feedback, and then pinpoint the bug and fix it – effectively the model learns to debug itself. This yielded state-of-the-art results on several coding tasks, improving accuracy by up to 12% on test-driven benchmarks by iteratively correcting errors.

% Reflexion, a recent idea for general reasoning, has the model generate a brief "reflec-
% tion" after a failed attempt, explaining what went wrong and how to adjust its next attempt. This reflection is then prepended to the next query so the model won't repeat the mistake. In coding, a similar approach can be used: after an unsuccessful run (say the model's code didn't produce a valid output or scored poorly), the agent can prompt the model to reflect: "Why did your solution perform poorly? What could be improved?" and use that answer to guide the next code revision.

% These self-corrective loops leverage the model's reasoning ability at inference time to gradually approach a correct solution, instead of having a perfect answer in one shot. 

% They are modular (generator + debugger) and don't require training new models – only cleverly designed prompt sequences and possibly a memory of past failures. This family of techniques is arguably one of the most promising for coding, because coding tasks naturally provide feedback (errors, test outcomes)
% that the model can learn from on the fly. 

% Search-Based Approaches (Beam Search, Tree-of-Thought, MCTS): Classic AI search algo-
% rithms can be applied at inference to guide the generation of code. Instead of treating the LLM as a black box that generates one solution, we can treat partial generations as states in a search tree. For instance, beam search is a technique where at each step of code generation, multiple possibilities are kept ("the beam") and the search explores several paths, not just the single most likely token sequence. In natural language generation this is tricky (due to the immense branching), but for structured tasks like code, researchers have attempted to incorporate it. A recent Hugging Face case study used beam search guided by a learned reward model to have a 1B model solve math problems step-by-step, expanding only the promising reasoning paths. 

% They further introduced Diverse Verifier Tree Search (DVTS), which explicitly encourages exploring diverse branches and uses a verifier to prune incorrect logic, preventing the search from converging on a wrong answer too early. Translating this to code: one could imagine generating different code approaches, one branch tries a neural network, another branch tries a gradient boosting model) and then evaluating both, continuing to refine whichever looks more promising. A search algorithm (DFS or BFS with pruning) explores combinations of these thoughts to find an overall successful solution. In coding tasks, a "thought" could be a high-level plan or a code snippet. By evaluating partial solutions (e.g., does the code run up to a certain point without error? Does a partial result look plausible?), an agent can systematically search for correct code. An Example is the S*framework augments parallel sampling with sequential debugging" – generating multiple initial programs, then iteratively debugging each using execution feedback, and then selecting the best program at the end. It even uses adaptive test generation during selection: the LLM is asked to come up with specific inputs that differentiate two candidate programs, then it runs both programs on those inputs to see which one fails. Such adaptive search is quite advanced – effectively using the model to adversarially test its own outputs – and it was shown to let a 3B coder outperform models as large as 70B on code benchmarks. 

% In summary, search-based ITS techniques treat code generation as a search problem, leveraging multiple candidates and systematic exploration. These methods require more
% complex control logic (to manage the search and comparisons) and sometimes an external judge (which can be the model itself in a verifier role), but they can dramatically improve coverage of the solution space for tricky coding tasks.

% Tool-Assisted and Modular Reasoning: Another angle for inference-time improvement is
% giving the model external tools or modules to use while solving the problem. In coding, the
% most pertinent tool is a Python interpreter or execution environment. An agent with tool use ReAct paradigm or API toolformer approaches can decide to run a piece of code to see
% what happens and incorporate the result into its next step. For example, if the task is to write a function, the agent could generate a candidate, execute it on some test inputs, and observe the outputs or any exceptions, then adjust accordingly. for general coding tasks, letting the model search error messages or look up API usage could help a smaller model correct mistakes that it wouldn't know how to fix from training data alone. 

% A modular approach could also mean splitting the problem among specialized modules: for instance, a static analyzer module could lint the model's code for syntax errors or bad practices, and a test generator module could produce unit tests for the code. These modules might themselves be LLM prompts
% or traditional programs. Their feedback can then be fed into the model's next inference. Because
% these happen at inference time (no weights changed, just additional computation/feedback), they
% fall under ITS. example for this is to integrate a type-checker: the model writes Python code, run a type checker or even a lightweight symbolic execution to catch obvious misuses, then prompt
% the model with "The following variables might be undefined." By incorporating such tools in the loop,
% we can elevate a smaller model's performance – it doesn't need to have all knowledge internally
% if it can query an external helper. The ReAct framework demonstrated that even a single LLM
% agent can use tools effectively by interleaving reasoning and acting steps. In coding, this could
% mean the agent alternates between "think about how to solve" and "try running this piece" actions.
% Overall, tool-assisted inference allows modularizing the task and providing grounded feedback

% Agentic Scaffolding and Role Decomposition: Closely related to the above, many agent
% frameworks split the problem-solving into multiple roles or phases. An example would be a planner-executor paradigm: first prompt the LLM to write a high-level plan or pseudo-code, then in a second prompt have it implement that plan in actual code. This helps because the model can focus on logical planning in one stage and syntax in another, reducing cognitive load. Another example is having a separate critic agent: one model (or one chain of prompts) produces a solution, and another model (or later prompts of the same model) evaluates that solution. For coding, one might have: a "data scientist" agent that decides what model to use, a "coder" agent that writes the code, and a "tester" agent that runs and verifies the code. All could be orchestrated in one loop. Such modular, agentic strategies can dramatically improve outcomes since mistakes caught by one module can be fixed before the final answer. They don't require additional training if we use the same base model with different prompts, though they
% incur more inference calls. This approach has been demonstrated in other domains (for example, decomposition into sub-agents for math word problems). 
% In code generation, the AutoKaggle system (2024) uses a multi-agent workflow with different specialists collaborating on a competition problem. While complex, this shows promise in handling the many facets of ML engineering (data cleaning, modeling, hyperparameter tuning) through coordinated inference-time work.
% In summary, inference-time scaling in coding tasks includes sampling-based methods (generate many solutions, then filter or vote), iterative improvement (let the model refine its answer using feedback,
% either self-generated or from executing code).

% \subsection{Machine Learning Engineering Benchmarks and Automated research workflow}

% coding benchmarks like swebench*
% agent scaffolds like AutoML (H2O) etc *
% AI scientist, and kaggle agents.*


% Background and Related Work
% This chapter provides the technical foundation necessary to understand our approach to enhancing open-source machine learning engineering agents through inference-time scaling. We begin by establishing key concepts in large language models and their inference mechanisms, then explore how inference-time scaling has proven effective across various domains, before examining the specific challenges and opportunities in ML engineering automation.
% 2.1 Large Language Models and Inference
% 2.1.1 Foundation Models and Training Paradigms
% Large Language Models (LLMs) are transformer-based neural networks trained on vast text corpora to predict the next token in a sequence. The training process typically involves three stages: pretraining on diverse internet text to learn general language understanding, supervised fine-tuning on curated instruction-following datasets to align with human preferences, and reinforcement learning from human feedback (RLHF) to further refine behavior through preference optimization techniques like PPO or DPO.
% The pretraining phase establishes the model's foundational capabilities—its understanding of syntax, semantics, factual knowledge, and reasoning patterns. Fine-tuning then shapes how these capabilities are expressed, teaching the model to follow instructions, engage in dialogue, and perform specific tasks. This multi-stage approach has proven crucial for creating models that are both knowledgeable and helpful.
% 2.1.2 Inference Mechanisms and Decoding Strategies
% During inference, LLMs generate text through autoregressive decoding, where each token is predicted based on all previous tokens in the sequence. The decoding process involves several key parameters that significantly influence output quality and diversity:
% Temperature controls the randomness of token selection, with lower values (0.1-0.3) producing more deterministic outputs and higher values (0.7-1.0) increasing creativity and variability. Top-k sampling restricts token selection to the k most probable candidates, while top-p (nucleus) sampling dynamically adjusts the candidate pool based on cumulative probability mass, typically set between 0.8-0.95 for optimal balance between coherence and diversity.
% Beam search represents an alternative decoding strategy that maintains multiple candidate sequences simultaneously, exploring different paths through the probability space. While computationally more expensive than sampling methods, beam search can produce higher-quality outputs for tasks requiring careful reasoning or structured responses.
% The choice of decoding strategy and parameters can dramatically affect model performance on complex tasks, making inference-time optimization a critical consideration for practical applications.
% 2.1.3 Inference Engines and Deployment
% Modern LLM deployment relies on specialized inference engines that optimize memory usage, computational efficiency, and throughput. Popular frameworks include vLLM, which uses PagedAttention for efficient memory management, TensorRT-LLM for NVIDIA GPU optimization, llama.cpp for CPU inference, and Text Generation Inference (TGI) for production deployments.
% These engines implement various optimization techniques including dynamic batching, continuous batching, and KV-cache management to maximize hardware utilization. The choice of inference engine can significantly impact both the speed and cost of serving LLMs, particularly important considerations when deploying open-source models at scale.
% 2.2 Inference-Time Scaling: Beyond Training Scale
% While the field has long focused on scaling models through increased parameters and training compute, inference-time scaling (ITS) offers an alternative paradigm where additional computation during inference enhances model capabilities without requiring larger models or extended training.
% 2.2.1 Core ITS Techniques
% Chain-of-Thought (CoT) Prompting emerged as one of the first successful ITS techniques, where models are prompted to "think step by step" and show their reasoning process explicitly. This simple modification has shown remarkable effectiveness across mathematical reasoning, logical puzzles, and complex problem-solving tasks, often improving performance by 20-50% on challenging benchmarks.
% Self-Consistency extends CoT by generating multiple reasoning paths for the same problem and selecting the most frequent answer through majority voting. This technique leverages the insight that correct reasoning paths tend to converge on the same solution, while errors are more likely to be diverse and inconsistent. Self-consistency has demonstrated particular strength in mathematical and logical reasoning tasks.
% Iterative Self-Debugging allows models to review and refine their own outputs through multiple rounds of generation and critique. The model first produces an initial solution, then analyzes it for potential errors, and finally generates an improved version. This process can be repeated multiple times, with each iteration potentially improving solution quality.
% Self-Reflection involves explicit metacognitive processes where models evaluate their own reasoning, identify potential weaknesses, and adjust their approach accordingly. Unlike self-debugging, which focuses on error correction, self-reflection emphasizes strategic thinking about problem-solving approaches.
% 2.2.2 ITS Success Across Domains
% The effectiveness of ITS techniques has been demonstrated across numerous domains. In mathematical reasoning, techniques like self-consistency have improved performance on GSM8K and MATH benchmarks by substantial margins. In code generation, iterative refinement approaches have shown significant improvements on programming challenges, with models able to debug and optimize their initial solutions.
% Scientific reasoning tasks have also benefited from ITS, with models showing improved performance on physics problems, chemistry questions, and biological reasoning when given additional inference-time computation. The pattern is consistent: domains that require careful reasoning and step-by-step analysis tend to benefit most from ITS techniques.
% Notably, these improvements often scale predictably with inference-time compute, following power-law relationships similar to those observed in training-time scaling. This suggests that ITS represents a fundamental trade-off between model size and inference cost, potentially democratizing access to high-performance AI capabilities.
% 2.3 Reasoning Models and Distillation
% 2.3.1 The DeepSeek-R1 Family
% DeepSeek-R1 represents a significant advancement in open-source reasoning models, trained specifically to excel at step-by-step reasoning and complex problem-solving. The model family includes variants at 7B, 14B, and 32B parameters, each distilled from larger teacher models while maintaining strong reasoning capabilities.
% The distillation process preserves the reasoning patterns and step-by-step thinking that make larger models effective, compressing these capabilities into more accessible model sizes. This approach has proven particularly effective for mathematical reasoning, code generation, and logical problem-solving—precisely the capabilities needed for effective ML engineering agents.
% 2.3.2 Structured Output Generation
% Modern reasoning models increasingly support structured output generation, allowing them to produce JSON, XML, or custom formatted responses reliably. This capability is crucial for agentic applications, where models must interact with external tools and APIs through precise data formats.
% Function calling and tool use capabilities enable models to interact with external systems programmatically, calling predefined functions with structured arguments and processing their returns. These capabilities are essential for ML engineering agents that must interact with databases, APIs, file systems, and computational environments.
% 2.4 Agentic Systems and Scaffolding
% 2.4.1 From Language Models to Autonomous Agents
% The transition from language models to autonomous agents represents one of the most significant developments in recent AI research. While language models excel at generating text responses to prompts, agents can plan multi-step actions, maintain state across interactions, and pursue complex goals autonomously.
% This transformation requires scaffolding—the software architecture that enables language models to interact with environments, maintain memory, and execute complex workflows. Scaffolding typically includes components for planning, memory management, tool integration, and error handling, creating a bridge between the model's language capabilities and real-world task execution.
% 2.4.2 Agent Architectures for Coding
% Coding agents face unique challenges compared to general-purpose agents. They must understand complex technical specifications, navigate large codebases, execute and debug code, and maintain consistency across multiple files and functions. Several architectural patterns have emerged to address these challenges:
% Tree-search based agents explore multiple solution paths simultaneously, using techniques borrowed from game-playing AI to navigate the space of possible implementations. Iterative refinement agents start with simple solutions and progressively add complexity through multiple rounds of generation and testing. Modular agents decompose large problems into smaller subproblems, solving each component independently before integration.
% 2.4.3 The AIDE Framework
% AIDE (AI-Driven Exploration) represents a sophisticated agent scaffolding specifically designed for data science and machine learning tasks. The framework implements a tree-search approach where agents explore multiple solution strategies, evaluate their effectiveness, and refine their approaches based on feedback from code execution and validation metrics.
% AIDE's architecture includes several key components: a planner that decomposes high-level goals into executable steps, an executor that runs code and captures results, a evaluator that assesses solution quality, and a memory system that maintains context across interactions. This modular design allows for systematic exploration of solution spaces while maintaining coherent progress toward task objectives.
% The original AIDE framework demonstrated impressive performance on data science competitions, but its reliance on general-purpose tree search and single-shot operation execution leaves room for enhancement through inference-time scaling techniques.
% 2.5 Machine Learning Engineering Benchmarks
% 2.5.1 The MLE-Bench Suite
% MLE-Bench represents the first comprehensive benchmark specifically designed to evaluate AI agents on end-to-end machine learning engineering tasks. Unlike traditional coding benchmarks that focus on algorithmic problems or simple function implementation, MLE-Bench requires agents to handle complete ML workflows including data preprocessing, model selection, hyperparameter optimization, and submission formatting.
% The benchmark comprises competitions from multiple ML domains including computer vision, natural language processing, tabular data analysis, time series forecasting, recommendation systems, and specialized domains like bioinformatics and materials science. Each competition provides realistic datasets, evaluation metrics, and submission requirements that mirror real-world ML engineering challenges.
% 2.5.2 Evaluation Methodology
% MLE-Bench evaluations involve several stages: setup where agents access competition data and requirements, exploration where they analyze datasets and plan approaches, implementation where they develop and test solutions, and submission where they format results according to competition requirements.
% Success is measured not only by final performance on held-out test sets but also by the agent's ability to navigate the complete workflow autonomously. This includes handling data quality issues, selecting appropriate evaluation strategies, and producing properly formatted submissions—capabilities that are essential for practical ML engineering but often overlooked in traditional benchmarks.
% 2.6 Research Gaps and Opportunities
% Despite significant progress in both ITS techniques and agentic systems, several key gaps remain that motivate our work:
% Limited Integration: While ITS techniques have proven effective across various domains, their systematic integration with agentic scaffolds remains under-explored. Most existing work applies ITS techniques in isolation rather than as part of comprehensive agent architectures.
% Open-Source Focus: The majority of advanced agentic systems rely on proprietary models accessed through APIs, creating barriers to accessibility and customization. Open-source implementations that can be deployed independently remain limited.
% ML Engineering Specificity: While coding agents have received significant attention, ML engineering presents unique challenges that require specialized approaches. The combination of data analysis, model development, and systematic experimentation demands capabilities beyond general programming.
% Systematic Evaluation: Comprehensive evaluation of different ITS techniques within agentic contexts, particularly for specialized domains like ML engineering, remains limited. Most studies focus on single techniques or narrow task domains.
% Our work addresses these gaps by systematically integrating multiple ITS techniques within an open-source agentic scaffold, specifically tailored for ML engineering tasks and evaluated comprehensively on realistic benchmarks. This approach enables us to explore fundamental questions about the effectiveness of inference-time scaling for complex, multi-step reasoning tasks while providing practical tools for the broader research community.
% % \end{document}
