\chapter{The Second Chapter}

When you get stuck, don't panic. 
The world is unlikely to end just now. 
Remember you can consult your supervisor, tutor, Nolu, Jan, 
and Simukai and Barry at agreed times. 

\begin{thm}[Jeff's Washing Theorem]
\label{thm:jwt}
If an item of clothing is too big, then washing it makes it bigger;
but if it is too small, washing it makes it smaller.
\end{thm}
\begin{proof}
Stated without proof. But a proof would look like this. 
\end{proof}

Notice that no Lemmas are required in the proof of Theorem \ref{thm:jwt}.

Use \textbackslash ref for tables, figures, theorems, etc. and \textbackslash eqref for equations.

Use \textbackslash ldots for continuation of commas $,\ldots,$ and \textbackslash cdots for continuation of operators $\times\cdots\times$.


\section{Literature Review}
% Content similar to OCR pages 6-8, 7-17, 6-16


\subsection{Overview of Large Language Models}
% Content from OCR page 6, 6
Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP), propelled by advancements in neural network architectures \cite{vaswani2017attention}. Early models relied on statistical methods; the introduction of RNNs and LSTMs \cite{hochreiter1997long} addressed some limitations. The Transformer architecture \cite{vaswani2017attention} marked a significant paradigm shift.

\subsection{Coding Agents}
% Content from OCR page 6, 6
Agent frameworks, also known as scaffolds split the problem-solving into multiple roles or phases. For instance, AIDE, ResearchAgent, CodeAct, each of which structures the LLM’s process differently. An example would be a \textbf{planner-executor paradigm}: first prompt the LLM to \textbf{write a high-level plan} or pseudo-code, then in a second prompt have it implement that plan in actual code. This helps because the model can focus on logical planning in one stage and syntax in another, reducing cognitive load. Another example is having a \textbf{separate critic agent}: one model (or one chain of prompts) produces a solution, and another model (or later prompts of the same model) evaluates that solution. 

% \subsection{AIDE Agent Framework and Inference-Time Scaling}
% \textbf{AIDE (AI-Driven Exploration)} is an agentic framework that uses large language models to solve machine learning tasks (notably Kaggle competitions) by generating and iteratively improving code solutions. The agent operates in a loop akin to how a human engineer would: it \textbf{generates an initial solution, runs it, debugs and evaluates it, then refines the code} based on feedback. This iterative optimization is a form of \textbf{inference-time scaling} – instead of producing an answer in one pass, AIDE allocates multiple inference steps for the model to “think” longer and improve its solution. A key element of AIDE’s current inference-time approach is its \textbf{self-reflection logic}, a two-step self-review process for code quality. In practice, after the model drafts a code solution, AIDE invokes a reflection routine that (1) \textbf{critiques the code and suggests minimal edits}, and (2) \textbf{applies those edits to produce a revised code}. This occurs at inference time (no further training), effectively giving the model a chance to correct obvious mistakes or inefficiencies before proceeding. If the reflection finds no issues (the critique might respond with \textit{“No specific errors found requiring changes.”}), AIDE knows the code likely doesn’t need minor fixes. Otherwise, the suggested improvements (provided as natural language instructions) are applied to get a refined solution, which AIDE can then execute. This self-reflection mechanism is inspired by human code review and is a lightweight way to boost correctness without an outside debugger. Notably, AIDE’s self-reflection is done by the same LLM (just prompted to act as a reviewer), so it \textbf{scales the “thinking time” of the model} by chaining model calls: first to generate code, then to analyze it, and finally to regenerate improved code.

% Beyond self-reflection, the AIDE framework performs full \textbf{solution testing and iterative refinement}. It doesn’t stop at one code draft – it is designed to \textbf{“iteratively run\textbackslash{}[], debug\textbackslash{}[], evaluate\textbackslash{}[], and improve\textbackslash{}[], the ML code, all by itself.”} In other words, AIDE actually executes the code on the given dataset (using the competition’s training/test data), observes the outcome (e.g. model performance or runtime errors), and feeds that feedback into subsequent model prompts. This aligns with AIDE’s \textit{Solution Space Tree Search} strategy described by its authors, where each cycle consists of: a \textbf{Solution Generator} (the LLM proposing a new or modified code), an \textbf{Evaluator} (running the code and checking its performance against the objective), and a \textbf{Solution Selector} (choosing the best solution or next direction based on results). In essence, AIDE is already leveraging inference-time compute by allowing multiple model iterations and code runs to converge on a good solution. Empirically, this approach has paid off: in an internal benchmark of 60+ Kaggle competitions, AIDE’s autonomous solutions on average outperformed \textbf{over 50\% of human contestants} – a striking result for a 7B-32B parameter model competing against human Kagglers. This demonstrates how a smaller model, given the chance to \textbf{reflect, execute, and refine multiple times}, can reach human-level performance on complex coding tasks.

% Importantly, the use of \textbf{inference-time scaling} (also called test-time or run-time scaling) is what enables AIDE to punch above the raw capability of its base model. Instead of relying solely on the pretrained knowledge of the LLM, AIDE’s framework \textbf{lets the model use more computation per query} – analogous to letting it “think longer” or explore several solution attempts. Experiments with inference-time scaling by \cite{nvidia_scaling} have similarly shown that even extremely large models benefit from taking more time to reason through a problem rather than answering in one go. AIDE’s self-reflection is one such reasoning step (focusing on code quality), and its iterative loop of generating and testing code is another (focusing on solution performance). Together, these strategies allow a model like DeepSeek-32B or smaller to approach problems methodically, narrowing the gap to much larger models that might solve it in a single shot. In summary, \textbf{AIDE’s agent framework uses structured prompts and tooling to turn a single-pass LLM into a multi-step problem solver}, with self-critiquing and trial-and-error at inference time to boost results.

\subsection{MLE-bench: Evaluating Coding Agents in ML Tasks}

To measure how well approaches like AIDE perform in the coding domain, \cite{mlebench} introduced \textbf{MLE-bench}, a benchmark specifically for \textbf{machine learning engineering tasks}. MLE-bench provides an \textbf{offline Kaggle-like environment} with 75 real competition problems (e.g. predicting house prices, classifying images, etc.), each including the competition description, datasets (training and hidden test data), an evaluation script to score submissions, and a snapshot of the human leaderboard for reference. The “coding domain” here means that an agent must produce \textbf{actual code (usually Python) to train a model and make predictions} for each task. The benchmark then runs that code in a sandbox (Docker) to generate a submission file and uses the provided script to compute the performance metric (such as accuracy, log-loss, RMSE, etc.). This setup mirrors the real workflow of Kaggle: read problem -> write training code -> get predictions -> score the result. Crucially, \textbf{the agent’s performance is evaluated by the same standards as human Kaggle competitors}. MLE-bench compares the agent’s score or ranking on each problem to Kaggle’s medal criteria: for example, if a competition had 500 teams, a top-100 score would be considered a “medal” (since Kaggle awards medals to roughly the top 20\% or top 100, whichever is smaller). By seeing how many competitions an agent can reach medal-level performance, we gauge its coding and ML problem-solving ability in a real-world sense.

\textbf{AIDE was the best-performing agent scaffold in MLE-bench}, according to OpenAI’s report. In evaluations using a powerful model (\cite{oai_o1preview}), the combination of \textbf{AIDE + o1-preview achieved medal-level results in 16.9\% of the 75 competitions}. This was about \textit{four times more medals} than the next-best agent framework evaluated, demonstrating AIDE’s effective design for these tasks. (For context, AIDE with GPT-4o managed \textasciitilde{}8.7\% and other agents with similar LLMs got even fewer medals.) It’s impressive that even with a relatively small model (OpenAI o1 is presumably much smaller than GPT-4), AIDE’s iterative approach was able to crack \textasciitilde{}12–13 competitions at a medal level. The \textbf{benchmark’s structure} makes these results meaningful: each agent had the same constraints (e.g. a fixed number of steps or a time limit, typically a few hours per task), and they were disallowed from cheating via internet access or training-data leaks. The evaluation focuses purely on \textit{coding performance} – can the agent produce a correct, high-performing solution via code. The MLE-bench framework even included checks like \textbf{Dolos plagiarism detection} \cite{dolos} to ensure the agent’s code wasn’t just regurgitating a human solution. In short, \textbf{MLE-bench treats AI agents as Kaggle competitors} and measures success by human-competitive standards (medals and ranks). AIDE’s strong showing indicates that its inference-time strategies (self-reflection, iterative debugging, etc.) translated into better code and model performance on these benchmarks.

It’s worth noting how MLE-bench executes and evaluates an agent. For each competition, the agent is given the problem description and data, then allowed to run until it produces a submission file (or time/step limit is reached). The \textbf{agent’s code is executed on the provided test set}, and the output predictions are scored by the competition’s metric script. This score is then compared to the distribution of human scores (from the public leaderboard) – effectively placing the agent on the leaderboard to see if it’d get a bronze medal, silver, etc. This approach to evaluation is very \textbf{outcome-focused}: it doesn’t directly grade the quality of code (the code could be messy as long as it works), but rather the \textbf{result of running that code}. Thus, an agent that can find a clever solution that trains a good model will score high, whereas an agent that writes syntactically correct code but with a poor modeling approach will score low. This is important in the context of inference-time scaling: sometimes an agent might generate code that runs without errors, but still yields a bad model. The benchmark forces the agent to confront that – ideally via an internal evaluation loop. AIDE’s design, which evaluates and improves its code based on validation performance, is well-suited to this setup. Indeed, AIDE essentially contains an inner loop similar to how MLE-bench judges final performance: it \textbf{runs the code and looks at the objective metric}, allowing it to discard or refine solutions that don’t perform well. This synergy between AIDE’s methodology and the benchmark’s evaluation method is likely a reason AIDE excelled relative to more naive frameworks. Overall, MLE-bench provides a rigorous, coding-centric measure of an agent’s capabilities, and it has confirmed that \textbf{inference-time strategies can greatly boost a small model’s real-world coding performance} (since a scaffold like AIDE let a 3B–32B model occasionally beat much larger models on these tasks).

\subsection{Inference-Time Scaling Techniques in the Coding Domain}

Inference-Time Scaling (ITS) refers to strategies that improve a model’s performance by expending more computation \textit{at inference time}, rather than relying on bigger model size or additional training. In the coding domain – tasks like code generation and problem solving – a variety of ITS techniques have been developed or proposed. The core idea is to allow a model to \textbf{generate and evaluate multiple solutions or reasoning steps} before finalizing an answer, much like a programmer iteratively writes, tests, and debugs code. This can \textbf{enable smaller models to achieve results on par with much larger models} by compensating with clever search and reasoning. Below is an overview of relevant ITS strategies in the context of code generation:
\subsection{Inference time scaling}
% Content from OCR pages 7-8, not applicable, not applicable
Entity knowledge involves understanding specific real-world entities. Explicit integration increases computational complexity. Work by \cite{hermann2015teaching} highlights challenges and methods like anonymization.

\subsection{Approaches to Improve models performance during inference}
% Content from OCR page 7, not applicable, not applicable
Strategies include integrating external knowledge bases (RAG) \cite{guu2020retrieval, lewis2020retrieval}, fact-checking modules \cite{zellers2019defending}, controlled text generation \cite{keskar2019ctrl}, and data curation \cite{gundavarapu2024machine}.

\subsection{Gaps in the Literature}
% Content from OCR page 8, not applicable, not applicable
Despite efforts, a gap exists regarding training LLMs without explicit entity knowledge. \cite{hermann2015teaching}'s work supports the notion that anonymized data can be effective. This study aims to fill this gap.
\clearpage % Start toc on a new page