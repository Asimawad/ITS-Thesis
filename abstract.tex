\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}


Problem and Motivation: The high cost and limited accessibility of proprietary large language models (LLMs) for machine learning engineering tasks has created a significant barrier for researchers and practitioners. While recent inference-time scaling techniques have shown promise in enhancing model capabilities without requiring larger parameters, their application to coding and ML engineering domains—particularly with open-source models—remains largely unexplored.
Approach: This work presents the first systematic implementation of inference-time scaling (ITS) strategies within an open-source agentic framework for ML engineering tasks. We augment the AIDE (AI-Driven Exploration) agent scaffold with multiple ITS techniques including self-consistency, iterative self-debugging, self-reflection, and modular task decomposition, specifically targeting DeepSeek-R1 distilled models (7B, 14B, and 32B parameters). We evaluate these enhanced agents on a curated subset of MLE-Bench competitions spanning diverse ML domains.
Key Findings: Our experiments reveal 10\% improvement in valid submission rate, 10\% increase in medal-winning performance. Notably, we observe that ITS effectiveness correlates strongly with model size—the 32B DeepSeek model shows substantial gains from our strategies, achieving performance comparable to GPT-4 Turbo, while smaller 7B models demonstrate limited improvement. Self-consistency and prompt chaining prove most effective, while self-reflection shows minimal gains, suggesting limitations in these models' self-correction capabilities. [PLACEHOLDER: Specific numerical comparisons with baselines]
Impact and Novelty: This work delivers the first open-source, accessible framework for ML engineering automation that demonstrates competitive performance with proprietary alternatives. We provide four distinct AIDE variants, each optimized for different ITS strategies, establishing a foundation for future research in open-source coding agents. Our findings offer crucial insights into the scaling behavior of inference-time techniques in coding domains, demonstrating that strategic computational allocation during inference can bridge the capability gap between open-source and proprietary models.
\vfill

\section*{Declaration}

I, the undersigned, hereby declare that the work contained in this research project is my original work, and that any work done by others or by myself previously has been acknowledged and referenced accordingly.
% \includegraphics[height=2cm]{images/signature.png} \newline \hrule

Asim Osman, 12 June 2025